{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a07e0b",
   "metadata": {
    "papermill": {
     "duration": 0.006548,
     "end_time": "2024-12-27T06:41:58.690261",
     "exception": false,
     "start_time": "2024-12-27T06:41:58.683713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195ce1a9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-27T06:41:58.704000Z",
     "iopub.status.busy": "2024-12-27T06:41:58.703455Z",
     "iopub.status.idle": "2024-12-27T06:42:10.166021Z",
     "shell.execute_reply": "2024-12-27T06:42:10.164724Z"
    },
    "papermill": {
     "duration": 11.4731,
     "end_time": "2024-12-27T06:42:10.168990",
     "exception": false,
     "start_time": "2024-12-27T06:41:58.695890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports for neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "# imports for vision tasks\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import pydicom\n",
    "\n",
    "# imports for preparing dataset\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import timm\n",
    "\n",
    "# imports for visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, Tuple, List, Optional, Union, Any\n",
    "import platform\n",
    "from enum import Enum\n",
    "import timm\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    precision_recall_curve, \n",
    "    auc,\n",
    "    accuracy_score,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94712da",
   "metadata": {
    "papermill": {
     "duration": 0.005439,
     "end_time": "2024-12-27T06:42:10.180504",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.175065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae7fcb",
   "metadata": {
    "papermill": {
     "duration": 0.005436,
     "end_time": "2024-12-27T06:42:10.191729",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.186293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Data Structure Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec9dcf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.205476Z",
     "iopub.status.busy": "2024-12-27T06:42:10.204707Z",
     "iopub.status.idle": "2024-12-27T06:42:10.214039Z",
     "shell.execute_reply": "2024-12-27T06:42:10.212510Z"
    },
    "papermill": {
     "duration": 0.018977,
     "end_time": "2024-12-27T06:42:10.216237",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.197260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpinalConditionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class with optimized device handling\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        images: torch.Tensor, \n",
    "        labels: torch.Tensor, \n",
    "        metadata: pd.DataFrame,\n",
    "        device: Optional[torch.device] = None\n",
    "    ):\n",
    "        if device is None:\n",
    "            self.device, self.device_type = get_best_available_device()\n",
    "        else:\n",
    "            self.device = device\n",
    "            self.device_type = device.type\n",
    "        \n",
    "        # Store as CPU tensors initially\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Move individual items to device when accessed\n",
    "        image = self.images[idx].to(self.device)\n",
    "        label = self.labels[idx].to(self.device)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87738a",
   "metadata": {
    "papermill": {
     "duration": 0.005428,
     "end_time": "2024-12-27T06:42:10.228782",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.223354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Premade Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909da6cd",
   "metadata": {
    "papermill": {
     "duration": 0.005354,
     "end_time": "2024-12-27T06:42:10.239854",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.234500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Getting Best Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19e374c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.253049Z",
     "iopub.status.busy": "2024-12-27T06:42:10.252612Z",
     "iopub.status.idle": "2024-12-27T06:42:10.261369Z",
     "shell.execute_reply": "2024-12-27T06:42:10.260205Z"
    },
    "papermill": {
     "duration": 0.018651,
     "end_time": "2024-12-27T06:42:10.264034",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.245383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_available_device() -> Tuple[torch.device, str]:\n",
    "    \"\"\"\n",
    "    Detect and return the best available device for tensor operations.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[torch.device, str]: Device object and device type string\n",
    "    \"\"\"\n",
    "    device_type = \"cpu\"\n",
    "    \n",
    "    # Check for CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "        \n",
    "    # Check for Apple M1/M2 MPS (Metal Performance Shaders)\n",
    "    elif platform.processor().startswith('arm') and platform.system() == 'Darwin' and \\\n",
    "         hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device_type = \"mps\"\n",
    "        \n",
    "    # Check for TPU (if torch_xla is available)\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        device_type = \"tpu\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "        \n",
    "    # Create device object based on type\n",
    "    if device_type == \"tpu\":\n",
    "        try:\n",
    "            device = xm.xla_device()\n",
    "        except NameError:\n",
    "            device = torch.device(\"cpu\")\n",
    "            device_type = \"cpu\"\n",
    "    else:\n",
    "        device = torch.device(device_type)\n",
    "        \n",
    "    return device, device_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a875ef5",
   "metadata": {
    "papermill": {
     "duration": 0.005121,
     "end_time": "2024-12-27T06:42:10.274989",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.269868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d68820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.287404Z",
     "iopub.status.busy": "2024-12-27T06:42:10.286984Z",
     "iopub.status.idle": "2024-12-27T06:42:10.297020Z",
     "shell.execute_reply": "2024-12-27T06:42:10.295744Z"
    },
    "papermill": {
     "duration": 0.019345,
     "end_time": "2024-12-27T06:42:10.299672",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.280327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_processed_dataset(base_load_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Load the processed dataset from disk.\n",
    "    \n",
    "    Args:\n",
    "        base_load_path (str): Path where the dataset is saved\n",
    "    \n",
    "    Returns:\n",
    "        dict: Loaded condition data\n",
    "        dict: Configuration data\n",
    "    \"\"\"\n",
    "    base_path = Path(base_load_path)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        raise ValueError(f\"Dataset path {base_path} does not exist\")\n",
    "    \n",
    "    # Load configuration\n",
    "    with open(base_path / 'config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Initialize condition data dictionary\n",
    "    loaded_condition_data = {}\n",
    "    \n",
    "    # Load data for each condition\n",
    "    for condition in config['core_conditions']:\n",
    "        condition_path = base_path / condition\n",
    "        \n",
    "        if condition_path.exists():\n",
    "            loaded_condition_data[condition] = {\n",
    "                'images': torch.load(condition_path / 'images.pt', weights_only=False),\n",
    "                'labels': torch.load(condition_path / 'labels.pt', weights_only=False),\n",
    "                'metadata': pd.read_pickle(condition_path / 'metadata.pkl')\n",
    "            }\n",
    "    \n",
    "    if verbose is True :\n",
    "        print(\"\\nDataset successfully loaded\")\n",
    "        print(\"\\nDataset Summary:\")\n",
    "        for condition, data in loaded_condition_data.items():\n",
    "            print(f\"\\n{condition}:\")\n",
    "            print(f\"Total samples: {len(data['images'])}\")\n",
    "            label_dist = torch.bincount(data['labels'])\n",
    "            for severity, idx in config['severity_mapping'].items():\n",
    "                if idx < len(label_dist):\n",
    "                    print(f\"  {severity}: {label_dist[idx].item()}\")\n",
    "    \n",
    "    return loaded_condition_data, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f1dac",
   "metadata": {
    "papermill": {
     "duration": 0.005728,
     "end_time": "2024-12-27T06:42:10.311366",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.305638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Splitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb4a96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.325353Z",
     "iopub.status.busy": "2024-12-27T06:42:10.324954Z",
     "iopub.status.idle": "2024-12-27T06:42:10.333499Z",
     "shell.execute_reply": "2024-12-27T06:42:10.332332Z"
    },
    "papermill": {
     "duration": 0.018157,
     "end_time": "2024-12-27T06:42:10.335715",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.317558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_train_val_split(\n",
    "    condition_data: Dict,\n",
    "    device: Optional[torch.device] = None,\n",
    "    val_ratio: float = 0.2,\n",
    "    seed: int = 42\n",
    ") -> Dict[str, Dict[str, Dict]]:\n",
    "    \"\"\"\n",
    "    Split the dataset into training and validation sets with automatic device support.\n",
    "    \n",
    "    Args:\n",
    "        condition_data (Dict): Dictionary containing data for each condition\n",
    "        device (Optional[torch.device]): Device to store the tensors on\n",
    "        val_ratio (float): Ratio of validation set size to total dataset size\n",
    "        seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Dictionary containing train and val splits for each condition\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    split_data = {\n",
    "        'train': {},\n",
    "        'val': {}\n",
    "    }\n",
    "    \n",
    "    for condition, data in condition_data.items():\n",
    "        dataset_size = len(data['images'])\n",
    "        val_size = int(dataset_size * val_ratio)\n",
    "        train_size = dataset_size - val_size\n",
    "        \n",
    "        # Create full dataset with device specification\n",
    "        full_dataset = SpinalConditionDataset(\n",
    "            data['images'],\n",
    "            data['labels'],\n",
    "            data['metadata'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            full_dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(seed)\n",
    "        )\n",
    "        \n",
    "        split_data['train'][condition] = train_dataset\n",
    "        split_data['val'][condition] = val_dataset\n",
    "        \n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a4d6a8",
   "metadata": {
    "papermill": {
     "duration": 0.005391,
     "end_time": "2024-12-27T06:42:10.346975",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.341584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Loader Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd93bf43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.360362Z",
     "iopub.status.busy": "2024-12-27T06:42:10.359945Z",
     "iopub.status.idle": "2024-12-27T06:42:10.368703Z",
     "shell.execute_reply": "2024-12-27T06:42:10.367568Z"
    },
    "papermill": {
     "duration": 0.018172,
     "end_time": "2024-12-27T06:42:10.370838",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.352666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    split_data: Dict[str, Dict[str, Dataset]],\n",
    "    device_type: str,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 4,\n",
    "    shuffle_train: bool = True\n",
    ") -> Dict[str, Dict[str, DataLoader]]:\n",
    "    \"\"\"\n",
    "    Create DataLoader objects optimized for the detected device.\n",
    "    \n",
    "    Args:\n",
    "        split_data (Dict): Dictionary containing train and val splits\n",
    "        device_type (str): Type of device being used\n",
    "        batch_size (int): Batch size for DataLoader\n",
    "        num_workers (int): Number of worker processes\n",
    "        shuffle_train (bool): Whether to shuffle training data\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Dictionary containing DataLoader objects for each split and condition\n",
    "    \"\"\"\n",
    "    dataloaders = {\n",
    "        'train': {},\n",
    "        'val': {}\n",
    "    }\n",
    "    \n",
    "    # Optimize DataLoader settings based on device\n",
    "    if device_type == \"cuda\":\n",
    "        pin_memory = False  # Data is already on GPU\n",
    "    elif device_type == \"tpu\":\n",
    "        pin_memory = False\n",
    "        num_workers = 0  # TPU often works better with synchronous loading\n",
    "    else:  # CPU or MPS\n",
    "        pin_memory = True\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        for condition, dataset in split_data[split].items():\n",
    "            dataloaders[split][condition] = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=(shuffle_train and split == 'train'),\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory\n",
    "            )\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca4846",
   "metadata": {
    "papermill": {
     "duration": 0.005301,
     "end_time": "2024-12-27T06:42:10.381916",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.376615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Getting Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c25ef29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.395092Z",
     "iopub.status.busy": "2024-12-27T06:42:10.394702Z",
     "iopub.status.idle": "2024-12-27T06:42:10.401762Z",
     "shell.execute_reply": "2024-12-27T06:42:10.400658Z"
    },
    "papermill": {
     "duration": 0.016485,
     "end_time": "2024-12-27T06:42:10.404006",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.387521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_device_specific_batch_size(device_type: str, base_batch_size: int) -> int:\n",
    "    \"\"\"\n",
    "    Adjust batch size based on device type and available memory.\n",
    "    \n",
    "    Args:\n",
    "        device_type (str): Type of device being used\n",
    "        base_batch_size (int): Requested batch size\n",
    "        \n",
    "    Returns:\n",
    "        int: Adjusted batch size\n",
    "    \"\"\"\n",
    "    if device_type == \"cuda\":\n",
    "        # Get available GPU memory and adjust batch size if needed\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        if gpu_memory < 8 * (1024**3):  # Less than 8GB\n",
    "            return min(base_batch_size, 16)\n",
    "    elif device_type == \"tpu\":\n",
    "        # TPUs often work better with larger batch sizes\n",
    "        return max(base_batch_size, 128)\n",
    "    \n",
    "    return base_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d74ad2",
   "metadata": {
    "papermill": {
     "duration": 0.005617,
     "end_time": "2024-12-27T06:42:10.415633",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.410016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Usage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e886abe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.429936Z",
     "iopub.status.busy": "2024-12-27T06:42:10.429087Z",
     "iopub.status.idle": "2024-12-27T06:42:10.440811Z",
     "shell.execute_reply": "2024-12-27T06:42:10.439516Z"
    },
    "papermill": {
     "duration": 0.022157,
     "end_time": "2024-12-27T06:42:10.443533",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.421376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(\n",
    "    base_load_path: str,\n",
    "    batch_size: int = 32,\n",
    "    val_ratio: float = 0.2,\n",
    "    num_workers: int = 4,\n",
    "    seed: int = 42,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[Dict, Dict, Dict, torch.device]:\n",
    "    \"\"\"\n",
    "    Load and prepare data loaders with automatic device detection and optimization.\n",
    "    \n",
    "    Args:\n",
    "        base_load_path (str): Path to dataset\n",
    "        batch_size (int): Base batch size for DataLoader\n",
    "        val_ratio (float): Validation set ratio\n",
    "        num_workers (int): Number of worker processes\n",
    "        seed (int): Random seed\n",
    "        verbose (bool): Whether to print dataset information\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - Dictionary of dataloaders\n",
    "        - Dictionary of split datasets\n",
    "        - Configuration dictionary\n",
    "        - Device being used\n",
    "    \"\"\"\n",
    "    # Detect best available device\n",
    "    device, device_type = get_best_available_device()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nUsing device: {device} ({device_type})\")\n",
    "    \n",
    "    # Adjust batch size for device\n",
    "    adjusted_batch_size = get_device_specific_batch_size(device_type, batch_size)\n",
    "    if verbose and adjusted_batch_size != batch_size:\n",
    "        print(f\"Adjusted batch size from {batch_size} to {adjusted_batch_size} for {device_type}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    condition_data, config = load_processed_dataset(base_load_path, verbose=verbose)\n",
    "    \n",
    "    # Create train/val split\n",
    "    split_data = create_train_val_split(\n",
    "        condition_data,\n",
    "        device=device,\n",
    "        val_ratio=val_ratio,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    dataloaders = create_dataloaders(\n",
    "        split_data,\n",
    "        device_type=device_type,\n",
    "        batch_size=adjusted_batch_size,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nDataset Summary:\")\n",
    "        for split in ['train', 'val']:\n",
    "            print(f\"\\n{split.capitalize()} set sizes:\")\n",
    "            for condition in config['core_conditions']:\n",
    "                if condition in split_data[split]:\n",
    "                    print(f\"{condition}: {len(split_data[split][condition])}\")\n",
    "        \n",
    "        # Print memory usage information\n",
    "        if device_type == \"cuda\":\n",
    "            print(\"\\nGPU Memory Usage:\")\n",
    "            print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            print(f\"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "        elif device_type == \"tpu\":\n",
    "            try:\n",
    "                import torch_xla.debug.metrics as met\n",
    "                print(\"\\nTPU Memory Usage:\")\n",
    "                print(met.metrics_report())\n",
    "            except ImportError:\n",
    "                pass\n",
    "    \n",
    "    return dataloaders, split_data, config, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df973d0c",
   "metadata": {
    "papermill": {
     "duration": 0.005941,
     "end_time": "2024-12-27T06:42:10.456018",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.450077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load & Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72352a75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:42:10.470493Z",
     "iopub.status.busy": "2024-12-27T06:42:10.469471Z",
     "iopub.status.idle": "2024-12-27T06:43:31.600512Z",
     "shell.execute_reply": "2024-12-27T06:43:31.599083Z"
    },
    "papermill": {
     "duration": 81.147791,
     "end_time": "2024-12-27T06:43:31.609979",
     "exception": false,
     "start_time": "2024-12-27T06:42:10.462188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu (cpu)\n",
      "\n",
      "Dataset successfully loaded\n",
      "\n",
      "Dataset Summary:\n",
      "\n",
      "spinal_canal_stenosis:\n",
      "Total samples: 9753\n",
      "  Normal/Mild: 8552\n",
      "  Moderate: 732\n",
      "  Severe: 469\n",
      "\n",
      "left_neural_foraminal_narrowing:\n",
      "Total samples: 9860\n",
      "  Normal/Mild: 7671\n",
      "  Moderate: 1792\n",
      "  Severe: 397\n",
      "\n",
      "right_neural_foraminal_narrowing:\n",
      "Total samples: 9829\n",
      "  Normal/Mild: 7684\n",
      "  Moderate: 1767\n",
      "  Severe: 378\n",
      "\n",
      "left_subarticular_stenosis:\n",
      "Total samples: 9603\n",
      "  Normal/Mild: 6857\n",
      "  Moderate: 1834\n",
      "  Severe: 912\n",
      "\n",
      "right_subarticular_stenosis:\n",
      "Total samples: 9612\n",
      "  Normal/Mild: 6862\n",
      "  Moderate: 1825\n",
      "  Severe: 925\n",
      "\n",
      "Dataset Summary:\n",
      "\n",
      "Train set sizes:\n",
      "spinal_canal_stenosis: 7803\n",
      "left_neural_foraminal_narrowing: 7888\n",
      "right_neural_foraminal_narrowing: 7864\n",
      "left_subarticular_stenosis: 7683\n",
      "right_subarticular_stenosis: 7690\n",
      "\n",
      "Val set sizes:\n",
      "spinal_canal_stenosis: 1950\n",
      "left_neural_foraminal_narrowing: 1972\n",
      "right_neural_foraminal_narrowing: 1965\n",
      "left_subarticular_stenosis: 1920\n",
      "right_subarticular_stenosis: 1922\n"
     ]
    }
   ],
   "source": [
    "dataloaders, split_data, config, device = load_and_prepare_data(\n",
    "    base_load_path=\"/kaggle/input/degenerative-spine-image-classificaton/processed_spine_dataset\",\n",
    "    batch_size=32,\n",
    "    val_ratio=0.2,\n",
    "    verbose=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9fefab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:43:31.624729Z",
     "iopub.status.busy": "2024-12-27T06:43:31.624285Z",
     "iopub.status.idle": "2024-12-27T06:43:31.632346Z",
     "shell.execute_reply": "2024-12-27T06:43:31.630910Z"
    },
    "papermill": {
     "duration": 0.018465,
     "end_time": "2024-12-27T06:43:31.634829",
     "exception": false,
     "start_time": "2024-12-27T06:43:31.616364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_validation_subset(val_data: Dataset, subset_size: int, seed: int = 42) -> Subset:\n",
    "    \"\"\"\n",
    "    Extract a subset from the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        val_data (Dataset): The validation dataset.\n",
    "        subset_size (int): The number of samples to include in the subset.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Subset: A subset of the validation dataset.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    indices = torch.randperm(len(val_data))[:subset_size]  # Randomly sample indices\n",
    "    return Subset(val_data, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e156a3bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:43:31.649337Z",
     "iopub.status.busy": "2024-12-27T06:43:31.648884Z",
     "iopub.status.idle": "2024-12-27T06:43:31.665520Z",
     "shell.execute_reply": "2024-12-27T06:43:31.664373Z"
    },
    "papermill": {
     "duration": 0.027103,
     "end_time": "2024-12-27T06:43:31.668312",
     "exception": false,
     "start_time": "2024-12-27T06:43:31.641209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define conditions\n",
    "conditions = [\n",
    "    \"spinal_canal_stenosis\",\n",
    "    \"left_neural_foraminal_narrowing\",\n",
    "    \"right_neural_foraminal_narrowing\",\n",
    "    \"left_subarticular_stenosis\",\n",
    "    \"right_subarticular_stenosis\"\n",
    "]\n",
    "\n",
    "subset_size = 100\n",
    "val_subset_loaders = {}\n",
    "\n",
    "for condition in conditions:\n",
    "    val_dataset = split_data['val'][condition]\n",
    "    val_subset = get_validation_subset(val_dataset, subset_size)\n",
    "    val_subset_loaders[condition] = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43dcc81",
   "metadata": {
    "papermill": {
     "duration": 0.005917,
     "end_time": "2024-12-27T06:43:31.680567",
     "exception": false,
     "start_time": "2024-12-27T06:43:31.674650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Spinal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77d2176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:43:31.695625Z",
     "iopub.status.busy": "2024-12-27T06:43:31.695209Z",
     "iopub.status.idle": "2024-12-27T06:43:31.718394Z",
     "shell.execute_reply": "2024-12-27T06:43:31.717113Z"
    },
    "papermill": {
     "duration": 0.034211,
     "end_time": "2024-12-27T06:43:31.720940",
     "exception": false,
     "start_time": "2024-12-27T06:43:31.686729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelArchitecture(str, Enum):\n",
    "    \"\"\"Supported model architectures\"\"\"\n",
    "    INCEPTION_V4 = \"inception_v4\"\n",
    "    EFFICIENTNET = \"efficientnet_b0\"\n",
    "    EFFICIENTNET_V2 = \"efficientnetv2_s\"\n",
    "    VGG16 = \"vgg16\"\n",
    "    VIT = \"vit\"  # Added Vision Transformer (ViT)\n",
    "\n",
    "\n",
    "class SpinalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for spinal condition classification with different backbone options.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        architecture: ModelArchitecture,\n",
    "        num_classes: int,\n",
    "        pretrained: bool = True,\n",
    "        dropout_rate: float = 0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the model with specified architecture.\n",
    "        \n",
    "        Args:\n",
    "            architecture (ModelArchitecture): Choice of model architecture\n",
    "            num_classes (int): Number of output classes\n",
    "            pretrained (bool): Whether to use pretrained weights\n",
    "            dropout_rate (float): Dropout rate for the final layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.architecture = architecture\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Get the best available device\n",
    "        self.device, self.device_type = get_best_available_device()\n",
    "        \n",
    "        # Create the backbone model\n",
    "        self.backbone = self._create_backbone(pretrained)\n",
    "        \n",
    "        # Get the number of features from the backbone\n",
    "        if architecture == ModelArchitecture.INCEPTION_V4:\n",
    "            num_features = 1536\n",
    "        elif architecture == ModelArchitecture.EFFICIENTNET:\n",
    "            num_features = 1280\n",
    "        elif architecture == ModelArchitecture.EFFICIENTNET_V2:\n",
    "            num_features = 1280\n",
    "        elif architecture == ModelArchitecture.VGG16:\n",
    "            num_features = 4096  # VGG16 outputs 4096 features\n",
    "        elif architecture == ModelArchitecture.VIT:\n",
    "            num_features = 768  # ViT base model outputs 768 features\n",
    "        \n",
    "        # Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Move model to the best available device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _create_backbone(self, pretrained: bool) -> nn.Module:\n",
    "        \"\"\"Create the backbone model based on the selected architecture.\"\"\"\n",
    "        pretrained_str = 'imagenet' if pretrained else None\n",
    "        \n",
    "        if self.architecture == ModelArchitecture.INCEPTION_V4:\n",
    "            model = timm.create_model('inception_v4.tf_in1k', pretrained=pretrained, num_classes=0)\n",
    "        elif self.architecture == ModelArchitecture.EFFICIENTNET:\n",
    "            model = timm.create_model('efficientnet_b0.ra4_e3600_r224_in1k', pretrained=pretrained, num_classes=0)\n",
    "        elif self.architecture == ModelArchitecture.EFFICIENTNET_V2:\n",
    "            model = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=pretrained, num_classes=0)\n",
    "        elif self.architecture == ModelArchitecture.VGG16:\n",
    "            model = timm.create_model('vgg16.tv_in1k', pretrained=pretrained, num_classes=0)\n",
    "        elif self.architecture == ModelArchitecture.VIT:\n",
    "            model = timm.create_model('vit_base_patch16_224.mae', pretrained=pretrained, num_classes=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported architecture: {self.architecture}\")\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        if x.device != self.device:\n",
    "            x = x.to(self.device)\n",
    "\n",
    "        if self.architecture == ModelArchitecture.VIT:\n",
    "            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    \n",
    "        # Get features from the backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Print the shape to debug\n",
    "        # print(\"Shape after backbone:\", features.shape)\n",
    "        \n",
    "        # Pass features through the classifier\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "        \n",
    "    def get_preprocessing_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the preprocessing parameters for the selected architecture.\"\"\"\n",
    "        imagenet_stats = {\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225]\n",
    "        }\n",
    "        \n",
    "        image_sizes = {\n",
    "            ModelArchitecture.INCEPTION_V4: 299,\n",
    "            ModelArchitecture.EFFICIENTNET: 224,\n",
    "            ModelArchitecture.EFFICIENTNET_V2: 256,\n",
    "            ModelArchitecture.VGG16: 224,  # VGG16 uses 224x224 input size\n",
    "            ModelArchitecture.VIT: 224     # ViT uses 224x224 input size\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'image_size': image_sizes[self.architecture],\n",
    "            'mean': imagenet_stats['mean'],\n",
    "            'std': imagenet_stats['std'],\n",
    "            'device': self.device,\n",
    "            'device_type': self.device_type\n",
    "        }\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    architecture: str,\n",
    "    num_classes: int,\n",
    "    pretrained: bool = True,\n",
    "    dropout_rate: float = 0.2,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[SpinalModel, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a model instance with the specified architecture.\n",
    "    \n",
    "    Args:\n",
    "        architecture (str): Name of the architecture to use\n",
    "        num_classes (int): Number of output classes\n",
    "        pretrained (bool): Whether to use pretrained weights\n",
    "        dropout_rate (float): Dropout rate for the final layer\n",
    "        verbose (bool): Whether to print device and model information\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[SpinalModel, Dict[str, Any]]: Model instance and preprocessing parameters\n",
    "    \"\"\"\n",
    "    # Validate and convert architecture string to enum\n",
    "    try:\n",
    "        arch = ModelArchitecture(architecture.lower())\n",
    "    except ValueError:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported architecture: {architecture}. \"\n",
    "            f\"Supported architectures: {[a.value for a in ModelArchitecture]}\"\n",
    "        )\n",
    "    \n",
    "    # Create model\n",
    "    model = SpinalModel(\n",
    "        architecture=arch,\n",
    "        num_classes=num_classes,\n",
    "        pretrained=pretrained,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    # Get preprocessing parameters\n",
    "    preprocess_params = model.get_preprocessing_parameters()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nModel Configuration:\")\n",
    "        print(f\"Architecture: {arch.value}\")\n",
    "        print(f\"Device: {preprocess_params['device']} ({preprocess_params['device_type']})\")\n",
    "        print(f\"Input size: {preprocess_params['image_size']}x{preprocess_params['image_size']}\")\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "        print(f\"Pretrained: {pretrained}\")\n",
    "        \n",
    "        # Print model summary if torchinfo is available\n",
    "        try:\n",
    "            from torchsummary import summary\n",
    "            # Assuming 'model' is your SpinalModel\n",
    "            input_size = (3, preprocess_params['image_size'], preprocess_params['image_size'])\n",
    "            summary(model, input_size=input_size)\n",
    "        except ImportError:\n",
    "            if verbose:\n",
    "                print(\"\\nInstall torchinfo for detailed model summary\")\n",
    "    \n",
    "    return model, preprocess_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00ea701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:43:31.735563Z",
     "iopub.status.busy": "2024-12-27T06:43:31.735108Z",
     "iopub.status.idle": "2024-12-27T06:43:39.432102Z",
     "shell.execute_reply": "2024-12-27T06:43:39.430728Z"
    },
    "papermill": {
     "duration": 7.70856,
     "end_time": "2024-12-27T06:43:39.435813",
     "exception": false,
     "start_time": "2024-12-27T06:43:31.727253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0689a5b6c24875949cae8e54b3e6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Configuration:\n",
      "Architecture: vit\n",
      "Device: cpu (cpu)\n",
      "Input size: 224x224\n",
      "Number of classes: 3\n",
      "Pretrained: True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "          Identity-2             [-1, 196, 768]               0\n",
      "        PatchEmbed-3             [-1, 196, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "          Identity-5             [-1, 197, 768]               0\n",
      "          Identity-6             [-1, 197, 768]               0\n",
      "         LayerNorm-7             [-1, 197, 768]           1,536\n",
      "            Linear-8            [-1, 197, 2304]       1,771,776\n",
      "          Identity-9          [-1, 12, 197, 64]               0\n",
      "         Identity-10          [-1, 12, 197, 64]               0\n",
      "           Linear-11             [-1, 197, 768]         590,592\n",
      "          Dropout-12             [-1, 197, 768]               0\n",
      "        Attention-13             [-1, 197, 768]               0\n",
      "         Identity-14             [-1, 197, 768]               0\n",
      "         Identity-15             [-1, 197, 768]               0\n",
      "        LayerNorm-16             [-1, 197, 768]           1,536\n",
      "           Linear-17            [-1, 197, 3072]       2,362,368\n",
      "             GELU-18            [-1, 197, 3072]               0\n",
      "          Dropout-19            [-1, 197, 3072]               0\n",
      "         Identity-20            [-1, 197, 3072]               0\n",
      "           Linear-21             [-1, 197, 768]       2,360,064\n",
      "          Dropout-22             [-1, 197, 768]               0\n",
      "              Mlp-23             [-1, 197, 768]               0\n",
      "         Identity-24             [-1, 197, 768]               0\n",
      "         Identity-25             [-1, 197, 768]               0\n",
      "            Block-26             [-1, 197, 768]               0\n",
      "        LayerNorm-27             [-1, 197, 768]           1,536\n",
      "           Linear-28            [-1, 197, 2304]       1,771,776\n",
      "         Identity-29          [-1, 12, 197, 64]               0\n",
      "         Identity-30          [-1, 12, 197, 64]               0\n",
      "           Linear-31             [-1, 197, 768]         590,592\n",
      "          Dropout-32             [-1, 197, 768]               0\n",
      "        Attention-33             [-1, 197, 768]               0\n",
      "         Identity-34             [-1, 197, 768]               0\n",
      "         Identity-35             [-1, 197, 768]               0\n",
      "        LayerNorm-36             [-1, 197, 768]           1,536\n",
      "           Linear-37            [-1, 197, 3072]       2,362,368\n",
      "             GELU-38            [-1, 197, 3072]               0\n",
      "          Dropout-39            [-1, 197, 3072]               0\n",
      "         Identity-40            [-1, 197, 3072]               0\n",
      "           Linear-41             [-1, 197, 768]       2,360,064\n",
      "          Dropout-42             [-1, 197, 768]               0\n",
      "              Mlp-43             [-1, 197, 768]               0\n",
      "         Identity-44             [-1, 197, 768]               0\n",
      "         Identity-45             [-1, 197, 768]               0\n",
      "            Block-46             [-1, 197, 768]               0\n",
      "        LayerNorm-47             [-1, 197, 768]           1,536\n",
      "           Linear-48            [-1, 197, 2304]       1,771,776\n",
      "         Identity-49          [-1, 12, 197, 64]               0\n",
      "         Identity-50          [-1, 12, 197, 64]               0\n",
      "           Linear-51             [-1, 197, 768]         590,592\n",
      "          Dropout-52             [-1, 197, 768]               0\n",
      "        Attention-53             [-1, 197, 768]               0\n",
      "         Identity-54             [-1, 197, 768]               0\n",
      "         Identity-55             [-1, 197, 768]               0\n",
      "        LayerNorm-56             [-1, 197, 768]           1,536\n",
      "           Linear-57            [-1, 197, 3072]       2,362,368\n",
      "             GELU-58            [-1, 197, 3072]               0\n",
      "          Dropout-59            [-1, 197, 3072]               0\n",
      "         Identity-60            [-1, 197, 3072]               0\n",
      "           Linear-61             [-1, 197, 768]       2,360,064\n",
      "          Dropout-62             [-1, 197, 768]               0\n",
      "              Mlp-63             [-1, 197, 768]               0\n",
      "         Identity-64             [-1, 197, 768]               0\n",
      "         Identity-65             [-1, 197, 768]               0\n",
      "            Block-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 2304]       1,771,776\n",
      "         Identity-69          [-1, 12, 197, 64]               0\n",
      "         Identity-70          [-1, 12, 197, 64]               0\n",
      "           Linear-71             [-1, 197, 768]         590,592\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "        Attention-73             [-1, 197, 768]               0\n",
      "         Identity-74             [-1, 197, 768]               0\n",
      "         Identity-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "         Identity-80            [-1, 197, 3072]               0\n",
      "           Linear-81             [-1, 197, 768]       2,360,064\n",
      "          Dropout-82             [-1, 197, 768]               0\n",
      "              Mlp-83             [-1, 197, 768]               0\n",
      "         Identity-84             [-1, 197, 768]               0\n",
      "         Identity-85             [-1, 197, 768]               0\n",
      "            Block-86             [-1, 197, 768]               0\n",
      "        LayerNorm-87             [-1, 197, 768]           1,536\n",
      "           Linear-88            [-1, 197, 2304]       1,771,776\n",
      "         Identity-89          [-1, 12, 197, 64]               0\n",
      "         Identity-90          [-1, 12, 197, 64]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "          Dropout-92             [-1, 197, 768]               0\n",
      "        Attention-93             [-1, 197, 768]               0\n",
      "         Identity-94             [-1, 197, 768]               0\n",
      "         Identity-95             [-1, 197, 768]               0\n",
      "        LayerNorm-96             [-1, 197, 768]           1,536\n",
      "           Linear-97            [-1, 197, 3072]       2,362,368\n",
      "             GELU-98            [-1, 197, 3072]               0\n",
      "          Dropout-99            [-1, 197, 3072]               0\n",
      "        Identity-100            [-1, 197, 3072]               0\n",
      "          Linear-101             [-1, 197, 768]       2,360,064\n",
      "         Dropout-102             [-1, 197, 768]               0\n",
      "             Mlp-103             [-1, 197, 768]               0\n",
      "        Identity-104             [-1, 197, 768]               0\n",
      "        Identity-105             [-1, 197, 768]               0\n",
      "           Block-106             [-1, 197, 768]               0\n",
      "       LayerNorm-107             [-1, 197, 768]           1,536\n",
      "          Linear-108            [-1, 197, 2304]       1,771,776\n",
      "        Identity-109          [-1, 12, 197, 64]               0\n",
      "        Identity-110          [-1, 12, 197, 64]               0\n",
      "          Linear-111             [-1, 197, 768]         590,592\n",
      "         Dropout-112             [-1, 197, 768]               0\n",
      "       Attention-113             [-1, 197, 768]               0\n",
      "        Identity-114             [-1, 197, 768]               0\n",
      "        Identity-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 3072]       2,362,368\n",
      "            GELU-118            [-1, 197, 3072]               0\n",
      "         Dropout-119            [-1, 197, 3072]               0\n",
      "        Identity-120            [-1, 197, 3072]               0\n",
      "          Linear-121             [-1, 197, 768]       2,360,064\n",
      "         Dropout-122             [-1, 197, 768]               0\n",
      "             Mlp-123             [-1, 197, 768]               0\n",
      "        Identity-124             [-1, 197, 768]               0\n",
      "        Identity-125             [-1, 197, 768]               0\n",
      "           Block-126             [-1, 197, 768]               0\n",
      "       LayerNorm-127             [-1, 197, 768]           1,536\n",
      "          Linear-128            [-1, 197, 2304]       1,771,776\n",
      "        Identity-129          [-1, 12, 197, 64]               0\n",
      "        Identity-130          [-1, 12, 197, 64]               0\n",
      "          Linear-131             [-1, 197, 768]         590,592\n",
      "         Dropout-132             [-1, 197, 768]               0\n",
      "       Attention-133             [-1, 197, 768]               0\n",
      "        Identity-134             [-1, 197, 768]               0\n",
      "        Identity-135             [-1, 197, 768]               0\n",
      "       LayerNorm-136             [-1, 197, 768]           1,536\n",
      "          Linear-137            [-1, 197, 3072]       2,362,368\n",
      "            GELU-138            [-1, 197, 3072]               0\n",
      "         Dropout-139            [-1, 197, 3072]               0\n",
      "        Identity-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "             Mlp-143             [-1, 197, 768]               0\n",
      "        Identity-144             [-1, 197, 768]               0\n",
      "        Identity-145             [-1, 197, 768]               0\n",
      "           Block-146             [-1, 197, 768]               0\n",
      "       LayerNorm-147             [-1, 197, 768]           1,536\n",
      "          Linear-148            [-1, 197, 2304]       1,771,776\n",
      "        Identity-149          [-1, 12, 197, 64]               0\n",
      "        Identity-150          [-1, 12, 197, 64]               0\n",
      "          Linear-151             [-1, 197, 768]         590,592\n",
      "         Dropout-152             [-1, 197, 768]               0\n",
      "       Attention-153             [-1, 197, 768]               0\n",
      "        Identity-154             [-1, 197, 768]               0\n",
      "        Identity-155             [-1, 197, 768]               0\n",
      "       LayerNorm-156             [-1, 197, 768]           1,536\n",
      "          Linear-157            [-1, 197, 3072]       2,362,368\n",
      "            GELU-158            [-1, 197, 3072]               0\n",
      "         Dropout-159            [-1, 197, 3072]               0\n",
      "        Identity-160            [-1, 197, 3072]               0\n",
      "          Linear-161             [-1, 197, 768]       2,360,064\n",
      "         Dropout-162             [-1, 197, 768]               0\n",
      "             Mlp-163             [-1, 197, 768]               0\n",
      "        Identity-164             [-1, 197, 768]               0\n",
      "        Identity-165             [-1, 197, 768]               0\n",
      "           Block-166             [-1, 197, 768]               0\n",
      "       LayerNorm-167             [-1, 197, 768]           1,536\n",
      "          Linear-168            [-1, 197, 2304]       1,771,776\n",
      "        Identity-169          [-1, 12, 197, 64]               0\n",
      "        Identity-170          [-1, 12, 197, 64]               0\n",
      "          Linear-171             [-1, 197, 768]         590,592\n",
      "         Dropout-172             [-1, 197, 768]               0\n",
      "       Attention-173             [-1, 197, 768]               0\n",
      "        Identity-174             [-1, 197, 768]               0\n",
      "        Identity-175             [-1, 197, 768]               0\n",
      "       LayerNorm-176             [-1, 197, 768]           1,536\n",
      "          Linear-177            [-1, 197, 3072]       2,362,368\n",
      "            GELU-178            [-1, 197, 3072]               0\n",
      "         Dropout-179            [-1, 197, 3072]               0\n",
      "        Identity-180            [-1, 197, 3072]               0\n",
      "          Linear-181             [-1, 197, 768]       2,360,064\n",
      "         Dropout-182             [-1, 197, 768]               0\n",
      "             Mlp-183             [-1, 197, 768]               0\n",
      "        Identity-184             [-1, 197, 768]               0\n",
      "        Identity-185             [-1, 197, 768]               0\n",
      "           Block-186             [-1, 197, 768]               0\n",
      "       LayerNorm-187             [-1, 197, 768]           1,536\n",
      "          Linear-188            [-1, 197, 2304]       1,771,776\n",
      "        Identity-189          [-1, 12, 197, 64]               0\n",
      "        Identity-190          [-1, 12, 197, 64]               0\n",
      "          Linear-191             [-1, 197, 768]         590,592\n",
      "         Dropout-192             [-1, 197, 768]               0\n",
      "       Attention-193             [-1, 197, 768]               0\n",
      "        Identity-194             [-1, 197, 768]               0\n",
      "        Identity-195             [-1, 197, 768]               0\n",
      "       LayerNorm-196             [-1, 197, 768]           1,536\n",
      "          Linear-197            [-1, 197, 3072]       2,362,368\n",
      "            GELU-198            [-1, 197, 3072]               0\n",
      "         Dropout-199            [-1, 197, 3072]               0\n",
      "        Identity-200            [-1, 197, 3072]               0\n",
      "          Linear-201             [-1, 197, 768]       2,360,064\n",
      "         Dropout-202             [-1, 197, 768]               0\n",
      "             Mlp-203             [-1, 197, 768]               0\n",
      "        Identity-204             [-1, 197, 768]               0\n",
      "        Identity-205             [-1, 197, 768]               0\n",
      "           Block-206             [-1, 197, 768]               0\n",
      "       LayerNorm-207             [-1, 197, 768]           1,536\n",
      "          Linear-208            [-1, 197, 2304]       1,771,776\n",
      "        Identity-209          [-1, 12, 197, 64]               0\n",
      "        Identity-210          [-1, 12, 197, 64]               0\n",
      "          Linear-211             [-1, 197, 768]         590,592\n",
      "         Dropout-212             [-1, 197, 768]               0\n",
      "       Attention-213             [-1, 197, 768]               0\n",
      "        Identity-214             [-1, 197, 768]               0\n",
      "        Identity-215             [-1, 197, 768]               0\n",
      "       LayerNorm-216             [-1, 197, 768]           1,536\n",
      "          Linear-217            [-1, 197, 3072]       2,362,368\n",
      "            GELU-218            [-1, 197, 3072]               0\n",
      "         Dropout-219            [-1, 197, 3072]               0\n",
      "        Identity-220            [-1, 197, 3072]               0\n",
      "          Linear-221             [-1, 197, 768]       2,360,064\n",
      "         Dropout-222             [-1, 197, 768]               0\n",
      "             Mlp-223             [-1, 197, 768]               0\n",
      "        Identity-224             [-1, 197, 768]               0\n",
      "        Identity-225             [-1, 197, 768]               0\n",
      "           Block-226             [-1, 197, 768]               0\n",
      "       LayerNorm-227             [-1, 197, 768]           1,536\n",
      "          Linear-228            [-1, 197, 2304]       1,771,776\n",
      "        Identity-229          [-1, 12, 197, 64]               0\n",
      "        Identity-230          [-1, 12, 197, 64]               0\n",
      "          Linear-231             [-1, 197, 768]         590,592\n",
      "         Dropout-232             [-1, 197, 768]               0\n",
      "       Attention-233             [-1, 197, 768]               0\n",
      "        Identity-234             [-1, 197, 768]               0\n",
      "        Identity-235             [-1, 197, 768]               0\n",
      "       LayerNorm-236             [-1, 197, 768]           1,536\n",
      "          Linear-237            [-1, 197, 3072]       2,362,368\n",
      "            GELU-238            [-1, 197, 3072]               0\n",
      "         Dropout-239            [-1, 197, 3072]               0\n",
      "        Identity-240            [-1, 197, 3072]               0\n",
      "          Linear-241             [-1, 197, 768]       2,360,064\n",
      "         Dropout-242             [-1, 197, 768]               0\n",
      "             Mlp-243             [-1, 197, 768]               0\n",
      "        Identity-244             [-1, 197, 768]               0\n",
      "        Identity-245             [-1, 197, 768]               0\n",
      "           Block-246             [-1, 197, 768]               0\n",
      "       LayerNorm-247             [-1, 197, 768]           1,536\n",
      "        Identity-248                  [-1, 768]               0\n",
      "         Dropout-249                  [-1, 768]               0\n",
      "        Identity-250                  [-1, 768]               0\n",
      "VisionTransformer-251                  [-1, 768]               0\n",
      "         Dropout-252                  [-1, 768]               0\n",
      "          Linear-253                  [-1, 512]         393,728\n",
      "            ReLU-254                  [-1, 512]               0\n",
      "         Dropout-255                  [-1, 512]               0\n",
      "          Linear-256                    [-1, 3]           1,539\n",
      "================================================================\n",
      "Total params: 86,041,859\n",
      "Trainable params: 86,041,859\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 479.06\n",
      "Params size (MB): 328.22\n",
      "Estimated Total Size (MB): 807.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model, preprocess_params = create_model(\n",
    "    architecture=\"vit\",  # or \"efficientnet\" or \"efficientnet_v2\"\n",
    "    num_classes=3,\n",
    "    pretrained=True,\n",
    "    verbose=True,\n",
    "    # unfreeze_layers=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817cc65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:43:39.459325Z",
     "iopub.status.busy": "2024-12-27T06:43:39.458206Z",
     "iopub.status.idle": "2024-12-27T06:43:46.245787Z",
     "shell.execute_reply": "2024-12-27T06:43:46.244101Z"
    },
    "papermill": {
     "duration": 6.800483,
     "end_time": "2024-12-27T06:43:46.248168",
     "exception": false,
     "start_time": "2024-12-27T06:43:39.447685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26/3512874719.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/kaggle/input/rsna-lumbar-dataset-vit-class-weight/checkpoints/checkpoint_best.pth', map_location=model.device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpinalModel(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('/kaggle/input/rsna-lumbar-dataset-vit-class-weight/checkpoints/checkpoint_best.pth', map_location=model.device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # If using for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945b8c6",
   "metadata": {
    "papermill": {
     "duration": 0.007141,
     "end_time": "2024-12-27T06:43:46.263826",
     "exception": false,
     "start_time": "2024-12-27T06:43:46.256685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f612e4a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:43:46.280254Z",
     "iopub.status.busy": "2024-12-27T06:43:46.279751Z",
     "iopub.status.idle": "2024-12-27T06:43:46.300369Z",
     "shell.execute_reply": "2024-12-27T06:43:46.298871Z"
    },
    "papermill": {
     "duration": 0.032022,
     "end_time": "2024-12-27T06:43:46.303061",
     "exception": false,
     "start_time": "2024-12-27T06:43:46.271039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(\n",
    "   model: nn.Module,\n",
    "   test_loaders: Dict[str, DataLoader],\n",
    "   num_classes: int,\n",
    "   save_dir: str,\n",
    "   threshold: float = 0.5,\n",
    "   device: Optional[torch.device] = None\n",
    ") -> Dict:\n",
    "   model.eval()\n",
    "   device = device or model.device\n",
    "   metrics = {}\n",
    "   \n",
    "   Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "   \n",
    "   for condition, loader in test_loaders.items():\n",
    "       all_preds, all_labels, all_probs, all_images = [], [], [], []\n",
    "       \n",
    "       with torch.no_grad():\n",
    "           for inputs, labels in loader:\n",
    "               inputs = inputs.to(device, non_blocking=True)\n",
    "               labels = labels.to(device, non_blocking=True)\n",
    "               \n",
    "               outputs = model(inputs)\n",
    "               probs = torch.softmax(outputs, dim=1)\n",
    "               preds = torch.argmax(probs, dim=1)\n",
    "               \n",
    "               all_preds.extend(preds.cpu().numpy())\n",
    "               all_probs.extend(probs.cpu().numpy())\n",
    "               all_labels.extend(labels.cpu().numpy())\n",
    "               all_images.extend(inputs.cpu().numpy())\n",
    "       \n",
    "       all_preds = np.array(all_preds)\n",
    "       all_probs = np.array(all_probs)\n",
    "       all_labels = np.array(all_labels)\n",
    "       all_images = np.array(all_images)\n",
    "       \n",
    "       # Visualization\n",
    "       num_samples = min(10, len(all_images))\n",
    "       fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "       \n",
    "       for i in range(num_samples):\n",
    "           img = all_images[i].transpose(1, 2, 0)\n",
    "           img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "           img = np.clip(img, 0, 1)\n",
    "           \n",
    "           axes[i, 0].imshow(img)\n",
    "           axes[i, 0].set_title(f'Image {i}')\n",
    "           \n",
    "           axes[i, 1].bar(range(num_classes), np.eye(num_classes)[all_labels[i]])\n",
    "           axes[i, 1].set_title(f'True: {all_labels[i]}')\n",
    "           \n",
    "           axes[i, 2].bar(range(num_classes), all_probs[i])\n",
    "           axes[i, 2].set_title(f'Pred: {all_preds[i]} ({all_probs[i][all_preds[i]]:.2f})')\n",
    "           \n",
    "       plt.tight_layout()\n",
    "       plt.savefig(os.path.join(save_dir, f'{condition}_results.png'))\n",
    "       plt.close()\n",
    "       \n",
    "       # Metrics\n",
    "       metrics[condition] = {\n",
    "           'accuracy': accuracy_score(all_labels, all_preds),\n",
    "           'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
    "           'per_class_accuracy': [],\n",
    "           'roc_auc_scores': [],\n",
    "           'pr_auc_scores': []\n",
    "       }\n",
    "       \n",
    "       for i in range(num_classes):\n",
    "           class_labels = (all_labels == i).astype(int)\n",
    "           class_probs = all_probs[:, i]\n",
    "           class_preds = (all_preds == i).astype(int)\n",
    "           \n",
    "           metrics[condition]['roc_auc_scores'].append(roc_auc_score(class_labels, class_probs))\n",
    "           precision, recall, _ = precision_recall_curve(class_labels, class_probs)\n",
    "           metrics[condition]['pr_auc_scores'].append(auc(recall, precision))\n",
    "           metrics[condition]['per_class_accuracy'].append(accuracy_score(class_labels, class_preds))\n",
    "       \n",
    "       metrics[condition].update({\n",
    "           'avg_roc_auc': np.mean(metrics[condition]['roc_auc_scores']),\n",
    "           'avg_pr_auc': np.mean(metrics[condition]['pr_auc_scores']),\n",
    "           'avg_accuracy': np.mean(metrics[condition]['per_class_accuracy'])\n",
    "       })\n",
    "       \n",
    "       print(f\"\\nResults for {condition}:\")\n",
    "       print(f\"Average ROC AUC: {metrics[condition]['avg_roc_auc']:.4f}\")\n",
    "       print(f\"Average PR AUC: {metrics[condition]['avg_pr_auc']:.4f}\")\n",
    "       print(f\"Average Accuracy: {metrics[condition]['avg_accuracy']:.4f}\")\n",
    "       print(\"\\nConfusion Matrix:\")\n",
    "       print(metrics[condition]['confusion_matrix'])\n",
    "   \n",
    "   return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51fb684e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:43:46.320455Z",
     "iopub.status.busy": "2024-12-27T06:43:46.319984Z",
     "iopub.status.idle": "2024-12-27T06:47:20.632995Z",
     "shell.execute_reply": "2024-12-27T06:47:20.631397Z"
    },
    "papermill": {
     "duration": 214.339548,
     "end_time": "2024-12-27T06:47:20.650416",
     "exception": false,
     "start_time": "2024-12-27T06:43:46.310868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for spinal_canal_stenosis:\n",
      "Average ROC AUC: 0.4707\n",
      "Average PR AUC: 0.3381\n",
      "Average Accuracy: 0.9133\n",
      "\n",
      "Confusion Matrix:\n",
      "[[87  0  0]\n",
      " [ 9  0  0]\n",
      " [ 4  0  0]]\n",
      "\n",
      "Results for left_neural_foraminal_narrowing:\n",
      "Average ROC AUC: 0.5855\n",
      "Average PR AUC: 0.4026\n",
      "Average Accuracy: 0.8867\n",
      "\n",
      "Confusion Matrix:\n",
      "[[83  0  0]\n",
      " [13  0  0]\n",
      " [ 4  0  0]]\n",
      "\n",
      "Results for right_neural_foraminal_narrowing:\n",
      "Average ROC AUC: 0.5828\n",
      "Average PR AUC: 0.3744\n",
      "Average Accuracy: 0.8467\n",
      "\n",
      "Confusion Matrix:\n",
      "[[77  0  0]\n",
      " [18  0  0]\n",
      " [ 5  0  0]]\n",
      "\n",
      "Results for left_subarticular_stenosis:\n",
      "Average ROC AUC: 0.5918\n",
      "Average PR AUC: 0.4141\n",
      "Average Accuracy: 0.8000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[70  0  0]\n",
      " [23  0  0]\n",
      " [ 7  0  0]]\n",
      "\n",
      "Results for right_subarticular_stenosis:\n",
      "Average ROC AUC: 0.7379\n",
      "Average PR AUC: 0.5116\n",
      "Average Accuracy: 0.7867\n",
      "\n",
      "Confusion Matrix:\n",
      "[[68  0  0]\n",
      " [22  0  0]\n",
      " [10  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "test_metrics = test_model(\n",
    "   model=model,\n",
    "   test_loaders=val_subset_loaders,\n",
    "   num_classes=3,\n",
    "   save_dir='./checkpoints'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c5b1a",
   "metadata": {
    "papermill": {
     "duration": 0.008527,
     "end_time": "2024-12-27T06:47:20.667195",
     "exception": false,
     "start_time": "2024-12-27T06:47:20.658668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5986261,
     "sourceId": 9772858,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 213256269,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 328.167549,
   "end_time": "2024-12-27T06:47:23.398825",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-27T06:41:55.231276",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0d7527fcdb744975bbd858850c1a0ce0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8ac5a2ad32f148d6a0e68a02d8097f49",
       "placeholder": "",
       "style": "IPY_MODEL_177ae9fa90fe4a0ab314f9e19629debe",
       "value": "model.safetensors:100%"
      }
     },
     "177ae9fa90fe4a0ab314f9e19629debe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "266fdb29d8e947869f91828f1ed20965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75427bc80fcd49f79bd3f76cf5848cc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7a6ac2bdc1d1462da433a6362971b483": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_266fdb29d8e947869f91828f1ed20965",
       "max": 343208550.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_75427bc80fcd49f79bd3f76cf5848cc1",
       "value": 343208550.0
      }
     },
     "8ac5a2ad32f148d6a0e68a02d8097f49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "94ac5c72bcdf47da90bd63e4d2989126": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d0689a5b6c24875949cae8e54b3e6cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0d7527fcdb744975bbd858850c1a0ce0",
        "IPY_MODEL_7a6ac2bdc1d1462da433a6362971b483",
        "IPY_MODEL_ba15de75a807486481b198c364e347b1"
       ],
       "layout": "IPY_MODEL_b31b348eebed4ce7ac4610e1c10653b2"
      }
     },
     "a09e499353f7449a922bff6c827b4f46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b31b348eebed4ce7ac4610e1c10653b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba15de75a807486481b198c364e347b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_94ac5c72bcdf47da90bd63e4d2989126",
       "placeholder": "",
       "style": "IPY_MODEL_a09e499353f7449a922bff6c827b4f46",
       "value": "343M/343M[00:04&lt;00:00,79.7MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
